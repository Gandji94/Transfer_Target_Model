{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4c5148",
   "metadata": {},
   "source": [
    "# Checking with evidently for data drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac3e220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from evidently import Report, Dataset, DataDefinition, BinaryClassification, MulticlassClassification\n",
    "from evidently.presets import DataDriftPreset, ClassificationPreset\n",
    "from evidently.metrics import ValueDrift\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import sys\n",
    "sys.path.append('..\\..\\src')\n",
    "import PY_Class_Def\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.metrics import brier_score_loss, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad979a1",
   "metadata": {},
   "source": [
    "## Setting up the data sets\n",
    "\n",
    "In the following step, we set up a reference (old/existing) dataset and a current (new) dataset.\n",
    "\n",
    "The goal of data drift detection is to assess whether the data distribution has changed sufficiently to justify model retraining. Avoiding unnecessary retraining when new data becomes available helps save both time and computational resources.\n",
    "\n",
    "Evidently performs feature-wise statistical comparisons between the reference and current datasets to identify potential data drift.\n",
    "\n",
    "In our case, we perform separate drift checks for each input feature, distinguishing between numerical and categorical variables:\n",
    "\n",
    "- Numerical features:\n",
    "\n",
    "    - Since the numerical features are not normally distributed and we do not make parametric assumptions, we use the Mann–Whitney U test, a non-parametric test that compares the distributions of two independent samples. For the checking the drift in prediction, we will use the Wassterstein method. This method will allow us to track continous numerical values (probability) and is sensetive to small changes.\n",
    "\n",
    "- Categorical features:\n",
    "\n",
    "    - Due to varying cardinality and sparsity, we apply different approaches:\n",
    "\n",
    "        - Chi-square tests for binary or low-cardinality categorical features where expected cell count assumptions are satisfied.\n",
    "\n",
    "        - Population Stability Index (PSI) and Jensen–Shannon divergence for high-cardinality or sparse categorical features, where chi-square assumptions may not hold. These metrics quantify the magnitude of distributional change rather than relying solely on statistical significance.\n",
    "\n",
    "        - Drift detection results are interpreted in combination with practical thresholds to decide whether retraining is warranted.\n",
    "\n",
    "Before we start, we need to establish first what will be considered as old/existing data and what as new.\n",
    "\n",
    "New Data:\n",
    "- The CSV file named \"Cleaned_Final_Stats_w_Rumors\", will be split into X_train, X_test, y_train and y_test\n",
    "\n",
    "Old/Existing Data:\n",
    "- We will use the CSV files stored in the \"Train_Test\" folder.\n",
    "\n",
    "This will allow us to compare if the training data has changed but also if there is a change in prediction.\n",
    "- Old train vs new data\n",
    "    - transfomed\n",
    "    - raw\n",
    "- Prediction will be maded on old train and new data\n",
    "    - entropy\n",
    "    - max conef\n",
    "    - margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99c5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(r'..\\..\\DataSources\\Processed\\Cleaned_Final_Stats_w_Rumors.csv')\n",
    "top_5={'Premier League', 'Ligue 1', 'Serie A','Bundesliga', 'LaLiga'}\n",
    "new_data['Other_or_Top']=new_data['League_Joined'].isin(top_5).astype(int)\n",
    "binary_model=joblib.load('..\\..\\\\Models\\\\Final_Model\\\\Binary\\\\binary_final_model_total_data_5_11_2025.pkl')\n",
    "binary_trhesh = joblib.load('..\\..\\\\Models\\\\Final_Model\\\\Binary\\\\best_thresh_binary.dict')\n",
    "multi_model=joblib.load('..\\..\\\\Models\\\\Final_Model\\\\Multiclass\\\\multiclass_final_model_total_data_8_11_2025.pkl')\n",
    "#we get avaiable classes\n",
    "le=LabelEncoder()\n",
    "\n",
    "#new data does not need to be split, it will just be transformed\n",
    "new_data_binary_ = new_data.copy()\n",
    "new_data_binary = new_data_binary_.drop(['Player','Squad','League','League_Joined','Club_Joined','Transfer_Fee','Other_or_Top'],axis=1)\n",
    "new_data_target_binary = new_data_binary_['Other_or_Top']\n",
    "\n",
    "new_data_multiclass_ = new_data_binary_[new_data_binary_['Other_or_Top'].eq(1)]\n",
    "new_data_multiclass = new_data_multiclass_.drop(['Player','Squad','League','League_Joined','Club_Joined','Transfer_Fee','Other_or_Top'],axis=1)\n",
    "new_data_target_multiclass = new_data_multiclass_['League_Joined']\n",
    "trans_new_target_multi = le.fit_transform(new_data_target_multiclass)\n",
    "\n",
    "#old data will be just seperated into train and test\n",
    "##note because I got carried away and saved the csv files via joblib's dump, therefore we need to load it via load\n",
    "old_train_binary = joblib.load('..\\..\\\\DataSources\\\\Model_Data\\\\Binary\\\\Train_Test\\\\X_Train_Data.CSV')\n",
    "old_test_binary = joblib.load('..\\..\\\\DataSources\\\\Model_Data\\\\Binary\\\\Train_Test\\\\X_Test_Data.CSV')\n",
    "old_target_train_binary = joblib.load('..\\..\\\\DataSources\\\\Model_Data\\\\Binary\\\\Train_Test\\\\y_Train_Data.CSV')\n",
    "old_target_test_binary = joblib.load('..\\..\\\\DataSources\\\\Model_Data\\\\Binary\\\\Train_Test\\\\y_Test_Data.CSV')\n",
    "\n",
    "old_train_multi = joblib.load('..\\..\\\\DataSources\\Model_Data\\Multiclass\\Train_Test\\X_Train_Data.CSV')\n",
    "old_test_multi = joblib.load('..\\..\\\\DataSources\\Model_Data\\Multiclass\\Train_Test\\X_Test_Data.CSV')\n",
    "old_target_train_multi = joblib.load('..\\..\\\\DataSources\\Model_Data\\Multiclass\\Train_Test\\y_Train_Data.CSV')\n",
    "old_target_test_multi = joblib.load('..\\..\\\\DataSources\\Model_Data\\Multiclass\\Train_Test\\y_Test_Data.CSV')\n",
    "\n",
    "#next we load the models with the existing pipeline structure and cut it down so we only have the transformation steps in the pipeline\n",
    "#by adding estimator, we can select different steps in the pipeline\n",
    "#--------------------------------------------------- Binary Data --------------------------------------------------------------------\n",
    "#we need to call \"calibrated_classifiers_\" directly otherwise we would just get an unfitted clone\n",
    "ccv=binary_model.calibrated_classifiers_[0].estimator\n",
    "check_is_fitted(ccv)\n",
    "binary_transform = ccv[:-2]\n",
    "trans_new_binary = binary_transform.transform(new_data_binary)\n",
    "trans_old_train_binary = binary_transform.transform(old_train_binary)\n",
    "trans_old_test_binary = binary_transform.transform(old_test_binary)\n",
    "\n",
    "#--------------------------------------------------- Multiclass Data --------------------------------------------------------------------\n",
    "#we need to call \"calibrated_classifiers_\" directly otherwise we would just get an unfitted clone\n",
    "ccv=multi_model.calibrated_classifiers_[0].estimator\n",
    "check_is_fitted(ccv)\n",
    "multi_transform = ccv[:-2]\n",
    "trans_new_multi = multi_transform.transform(new_data_multiclass)\n",
    "trans_old_train_multi = multi_transform.transform(old_train_multi)\n",
    "trans_old_test_multi = multi_transform.transform(old_test_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d95589",
   "metadata": {},
   "source": [
    "## Setting up evidently to check for drift in the training data, test data and prediction\n",
    "\n",
    "### Data drift in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae144243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------- Binary Report -------------------------------------\n",
    "##--------------------------------------- Raw Binary Data Drift ------------------------------------\n",
    "report = Report(metrics=[DataDriftPreset(\n",
    "    #js => Jensen-Shannon will be used for categorical and binary values\n",
    "    cat_method = 'jensenshannon', cat_threshold = 0.1,\n",
    "    num_method='mannw', num_threshold=0.05,\n",
    "    #threshold for data drift, when surpassed, then column will be considered as drifted\n",
    "    drift_share=0.2\n",
    "\n",
    ")])\n",
    "report_binary_drift = report.run(reference_data=old_train_binary, current_data=new_data_binary)\n",
    "report_binary_drift.save_html('Binary_Raw_Data_Drift_Detection.html')\n",
    "\n",
    "##--------------------------------------- Transformed Binary Data Drift ------------------------------------\n",
    "report = Report(metrics=[DataDriftPreset(\n",
    "    #js => Jensen-Shannon will be used for categorical and binary values\n",
    "    cat_method = 'jensenshannon', cat_threshold = 0.1,\n",
    "    num_method='psi', num_threshold=0.05,\n",
    "    #threshold for data drift, when surpassed, then column will be considered as drifted\n",
    "    drift_share=0.2\n",
    "\n",
    ")])\n",
    "report_binary_drift = report.run(reference_data=trans_old_train_binary, current_data=trans_new_binary)\n",
    "report_binary_drift.save_html('Binary_Transformed_Data_Drift_Detection.html')\n",
    "\n",
    "#---------------------------------------- Multiclass Report -------------------------------------\n",
    "##--------------------------------------- Raw Multiclass Data Drift ------------------------------------\n",
    "report = Report(metrics=[DataDriftPreset(\n",
    "    #js => Jensen-Shannon will be used for categorical and binary values\n",
    "    cat_method = 'jensenshannon', cat_threshold = 0.1,\n",
    "    num_method='mannw', num_threshold=0.05,\n",
    "    #threshold for data drift, when surpassed, then column will be considered as drifted\n",
    "    drift_share=0.2\n",
    "\n",
    ")])\n",
    "report_binary_drift = report.run(reference_data=trans_old_train_multi, current_data=trans_new_multi)\n",
    "report_binary_drift.save_html('Multiclass_Raw_Data_Drift_Detection.html')\n",
    "\n",
    "##--------------------------------------- Transformed Multiclass Data Drift ------------------------------------\n",
    "report = Report(metrics=[DataDriftPreset(\n",
    "    #js => Jensen-Shannon will be used for categorical and binary values\n",
    "    cat_method = 'jensenshannon', cat_threshold = 0.1,\n",
    "    num_method='psi', num_threshold=0.05,\n",
    "    drift_share=0.2\n",
    "\n",
    ")])\n",
    "report_multiclass_drift = report.run(reference_data=trans_old_train_multi, current_data=trans_new_multi)\n",
    "report_multiclass_drift.save_html('Multiclass_Transformed_Data_Drift_Detection.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f722f3d",
   "metadata": {},
   "source": [
    "### Data drift in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30b4257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_old_pred = binary_model.predict_proba(old_train_binary)\n",
    "binary_new_pred = binary_model.predict_proba(new_data_binary)\n",
    "\n",
    "#with ravel() we flatten the array, so we can check if there is a drift\n",
    "#in class 0 and class 1\n",
    "binary_old_pred_df = pd.DataFrame({\n",
    "    \"p_class_1\": binary_old_pred[:,1]\n",
    "})\n",
    "binary_new_pred_df = pd.DataFrame({\n",
    "    \"p_class_1\": binary_new_pred[:,1]\n",
    "})\n",
    "\n",
    "multi_old_pred = multi_model.predict_proba(old_train_multi)\n",
    "multi_new_pred = multi_model.predict_proba(new_data_multiclass)\n",
    "\n",
    "#with ravel() we flatten the array, so we can check if there is a drift\n",
    "#in all 5 classes\n",
    "multi_old_pred_df = pd.DataFrame({f'p_class_{i}':multi_old_pred[:,i] for i in range(0,len(le.classes_))})\n",
    "\n",
    "multi_new_pred_df = pd.DataFrame({f'p_class_{i}':multi_new_pred[:,i] for i in range(0,len(le.classes_))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[DataDriftPreset(\n",
    "    #wasserstein distance is used for numerical features\n",
    "    #it works well for continuous probability outputs and\n",
    "    #is sensitive to subtle distribution and calibration shifts\n",
    "    num_method = 'wasserstein', cat_threshold = 0.1,\n",
    "    #threshold for data drift, when surpassed, then column will be considered as drifted\n",
    "    #for the binary class we select a drif_share of 1.0, this means we need a shift of a whole class\n",
    "    #to trigger a drift notification\n",
    "    drift_share=1.0),\n",
    "    #THIS COLUMN IS OPTIONAL, WE ONLY CHECK ONE COLUMN\n",
    "    ValueDrift(column=\"p_class_1\", method=\"wasserstein\", threshold=0.1)\n",
    "])\n",
    "report_binary_drift = report.run(reference_data=binary_old_pred_df, current_data=binary_new_pred_df)\n",
    "report_binary_drift.save_html('Drift in Binary Prediction.html')\n",
    "\n",
    "\n",
    "#in this example we take half of all avaiable classes and floor it\n",
    "#in our example we would trigger an alert, when 2 of 5 classes shifted\n",
    "min_number_of_classes_drift = floor((len(le.classes_)*0.5))\n",
    "num_of_classes = len(le.classes_)\n",
    "multi_drift_share = min_number_of_classes_drift/num_of_classes\n",
    "\n",
    "report = Report(metrics=[DataDriftPreset(\n",
    "    #wasserstein distance is used for numerical features\n",
    "    #it works well for continuous probability outputs and\n",
    "    #is sensitive to subtle distribution and calibration shifts\n",
    "    num_method = 'wasserstein', cat_threshold = 0.1,\n",
    "    #threshold for data drift, when surpassed, then column will be considered as drifted\n",
    "    drift_share=multi_drift_share\n",
    "\n",
    ")])\n",
    "report_multi_drift = report.run(reference_data=multi_old_pred_df, current_data=multi_new_pred_df)\n",
    "report_multi_drift.save_html('Drift in Multi Prediction.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ab0fc",
   "metadata": {},
   "source": [
    "### Comparing Overall Metric Scores old vs new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0f4595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Brier Score: 0.1298205588718516\n",
      "\n",
      "New Brier Score: 0.1310089005077629\n",
      "\n",
      "Old Multi-Log-Loss Score: 1.022553311161437\n",
      "\n",
      "New Multi-Log-Loss Score: 1.0181834648127581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_brier_score = brier_score_loss(old_target_train_binary,binary_old_pred[:,1])\n",
    "print(f'Old Brier Score: {old_brier_score}\\n')\n",
    "new_brier_score = brier_score_loss(new_data_target_binary,binary_new_pred[:,1])\n",
    "print(f'New Brier Score: {new_brier_score}\\n')\n",
    "\n",
    "old_log_loss = log_loss(old_target_train_multi, multi_old_pred)\n",
    "print(f'Old Multi-Log-Loss Score: {old_log_loss}\\n')\n",
    "new_log_loss = log_loss(trans_new_target_multi, multi_new_pred)\n",
    "print(f'New Multi-Log-Loss Score: {new_log_loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e4149",
   "metadata": {},
   "source": [
    "### Shift in Label Predictions\n",
    "\n",
    "NOTE in this project, it is not really needed, because we only showcase probabilities of the different classes. But for illustration purposes, we will do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4be11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building prediction dataframe\n",
    "old_binary_proba = binary_model.predict_proba(old_train_binary)[:, 1]\n",
    "old_binary_pred  = (old_binary_proba >= binary_trhesh[\"appropriate_prob_thresh\"]).astype(int)\n",
    "\n",
    "old_pred_df = pd.DataFrame({\n",
    "    \"target\": old_target_train_binary,\n",
    "    \"prob_class_1\": old_binary_proba,\n",
    "    \"pred\": old_binary_pred,\n",
    "})\n",
    "\n",
    "new_binary_proba = binary_model.predict_proba(new_data_binary)[:, 1]\n",
    "new_binary_pred  = (new_binary_proba >= binary_trhesh[\"appropriate_prob_thresh\"]).astype(int)\n",
    "\n",
    "new_pred_df = pd.DataFrame({\n",
    "    \"target\": new_data_target_binary,\n",
    "    \"prob_class_1\": new_binary_proba,\n",
    "    \"pred\": new_binary_pred,\n",
    "})\n",
    "\n",
    "#here we setup the data definition\n",
    "data_def = DataDefinition(\n",
    "    classification=[\n",
    "        BinaryClassification(\n",
    "            target=\"target\",\n",
    "            prediction_labels=\"pred\",        #hard label column\n",
    "            prediction_probas=\"prob_class_1\",#probability column\n",
    "            pos_label=1\n",
    "        )\n",
    "    ],\n",
    "    categorical_columns=[\"target\", \"pred\"]  #help Evidently treat them correctly\n",
    ")\n",
    "\n",
    "#creating a dataset\n",
    "ref_ds = Dataset.from_pandas(old_pred_df, data_definition=data_def)\n",
    "cur_ds = Dataset.from_pandas(new_pred_df, data_definition=data_def)\n",
    "\n",
    "report = Report([ClassificationPreset()], include_tests=True)\n",
    "binary_quality_drift = report.run(current_data=cur_ds, reference_data=ref_ds)\n",
    "binary_quality_drift.save_html(\"Binary_Quality_Drift.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b63a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(le.classes_)\n",
    "proba_cols = [str(i) for i in range(K)]  # proba column names are strings\n",
    "\n",
    "#the def-function is added to the \"PY_Class_Def\" file\n",
    "def add_missing_class_rows(df: pd.DataFrame, proba_cols, K: int):\n",
    "    \"\"\"\n",
    "    Add 1 dummy row per missing target class to avoid Evidently division-by-zero\n",
    "    when a class has 0 support in `target`.\n",
    "    IMPORTANT: labels must be strings to match proba_cols.\n",
    "    \"\"\"\n",
    "    present = set(df[\"target\"].unique())          # strings\n",
    "    missing = sorted(set(proba_cols) - present, key=int)\n",
    "\n",
    "    if not missing:\n",
    "        return df\n",
    "\n",
    "    extra = []\n",
    "    for c in missing:\n",
    "        row = {col: 0.0 for col in proba_cols}\n",
    "        row[c] = 1.0\n",
    "        row[\"pred\"] = c\n",
    "        row[\"target\"] = c\n",
    "        extra.append(row)\n",
    "\n",
    "    return pd.concat([df, pd.DataFrame(extra)], ignore_index=True)\n",
    "\n",
    "\n",
    "# -------------------- OLD (reference) --------------------\n",
    "old_multi_proba = multi_model.predict_proba(old_train_multi)\n",
    "old_multi_pred  = np.argmax(old_multi_proba, axis=1)\n",
    "\n",
    "old_pred_df = pd.DataFrame(old_multi_proba, columns=proba_cols)\n",
    "old_pred_df[\"pred\"] = old_multi_pred\n",
    "old_pred_df[\"target\"] = old_target_train_multi\n",
    "\n",
    "# FORCE CONSISTENT LABEL SPACE: everything as string labels \"0\",\"1\",...\n",
    "old_pred_df[\"pred\"] = old_pred_df[\"pred\"].astype(int).astype(str)\n",
    "old_pred_df[\"target\"] = old_pred_df[\"target\"].astype(int).astype(str)\n",
    "\n",
    "old_pred_df = add_missing_class_rows(old_pred_df, proba_cols, K)\n",
    "\n",
    "\n",
    "# -------------------- NEW (current) --------------------\n",
    "new_multi_proba = multi_model.predict_proba(new_data_multiclass)\n",
    "new_multi_pred  = np.argmax(new_multi_proba, axis=1)\n",
    "\n",
    "new_pred_df = pd.DataFrame(new_multi_proba, columns=proba_cols)\n",
    "new_pred_df[\"pred\"] = new_multi_pred\n",
    "new_pred_df[\"target\"] = trans_new_target_multi\n",
    "\n",
    "# FORCE CONSISTENT LABEL SPACE: everything as string labels \"0\",\"1\",...\n",
    "new_pred_df[\"pred\"] = new_pred_df[\"pred\"].astype(int).astype(str)\n",
    "new_pred_df[\"target\"] = new_pred_df[\"target\"].astype(int).astype(str)\n",
    "\n",
    "new_pred_df = add_missing_class_rows(new_pred_df, proba_cols, K)\n",
    "\n",
    "\n",
    "# -------------------- Evidently Definition --------------------\n",
    "multi_data_def = DataDefinition(\n",
    "    classification=[\n",
    "        MulticlassClassification(\n",
    "            target=\"target\",\n",
    "            prediction_labels=\"pred\",\n",
    "            prediction_probas=proba_cols\n",
    "        )\n",
    "    ],\n",
    "    categorical_columns=[\"target\", \"pred\"]\n",
    ")\n",
    "\n",
    "ref_ds = Dataset.from_pandas(old_pred_df, data_definition=multi_data_def)\n",
    "cur_ds = Dataset.from_pandas(new_pred_df, data_definition=multi_data_def)\n",
    "\n",
    "report = Report([ClassificationPreset()], include_tests=True)\n",
    "multi_quality_drift = report.run(current_data=cur_ds, reference_data=ref_ds)\n",
    "multi_quality_drift.save_html(\"Multi_Quality_Drift.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb19616",
   "metadata": {},
   "source": [
    "### Calibration-ish drift without labels (entropy + confidence drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d269f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_from_proba(P: np.ndarray) -> np.ndarray:\n",
    "    EPS = 1e-12\n",
    "    # P: (n, K), rows sum to 1\n",
    "    P = np.clip(P, EPS, 1.0)\n",
    "    return -np.sum(P * np.log(P), axis=1)\n",
    "\n",
    "def max_confidence(P: np.ndarray) -> np.ndarray:\n",
    "    return np.max(P, axis=1)\n",
    "\n",
    "# old/new probabilities\n",
    "old_p1 = binary_model.predict_proba(old_train_binary)[:, 1]\n",
    "new_p1 = binary_model.predict_proba(new_data_binary)[:, 1]\n",
    "\n",
    "# convert to (n,2) to compute entropy consistently\n",
    "old_P = np.vstack([1 - old_p1, old_p1]).T\n",
    "new_P = np.vstack([1 - new_p1, new_p1]).T\n",
    "\n",
    "ref_df = pd.DataFrame({\n",
    "    \"p_class_1\": old_p1,\n",
    "    \"entropy\": entropy_from_proba(old_P),\n",
    "    \"max_conf\": max_confidence(old_P),\n",
    "})\n",
    "\n",
    "cur_df = pd.DataFrame({\n",
    "    \"p_class_1\": new_p1,\n",
    "    \"entropy\": entropy_from_proba(new_P),\n",
    "    \"max_conf\": max_confidence(new_P),\n",
    "})\n",
    "\n",
    "report = Report([\n",
    "    DataDriftPreset(num_method=\"wasserstein\", drift_share=0.34),  # 1 of 3 cols drifting triggers\n",
    "    ValueDrift(column=\"entropy\", method=\"wasserstein\", threshold=0.1),\n",
    "    ValueDrift(column=\"max_conf\", method=\"wasserstein\", threshold=0.1),\n",
    "])\n",
    "\n",
    "binary_entropy_drift = report.run(cur_df, ref_df)\n",
    "binary_entropy_drift.save_html(\"Binary_Confidence_Entropy_Drift.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05adfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_P = multi_model.predict_proba(old_train_multi)        # (n, K)\n",
    "new_P = multi_model.predict_proba(new_data_multiclass)\n",
    "\n",
    "proba_cols = [f\"p_class_{i}\" for i in range(old_P.shape[1])]\n",
    "\n",
    "ref_df = pd.DataFrame(old_P, columns=proba_cols)\n",
    "ref_df[\"entropy\"] = entropy_from_proba(old_P)\n",
    "ref_df[\"max_conf\"] = max_confidence(old_P)\n",
    "\n",
    "cur_df = pd.DataFrame(new_P, columns=proba_cols)\n",
    "cur_df[\"entropy\"] = entropy_from_proba(new_P)\n",
    "cur_df[\"max_conf\"] = max_confidence(new_P)\n",
    "\n",
    "report = Report([\n",
    "    DataDriftPreset(num_method=\"wasserstein\", drift_share=0.4),\n",
    "    ValueDrift(column=\"entropy\", method=\"wasserstein\", threshold=0.1),\n",
    "    ValueDrift(column=\"max_conf\", method=\"wasserstein\", threshold=0.1),\n",
    "])\n",
    "\n",
    "multi_entropy_drift = report.run(cur_df, ref_df)\n",
    "multi_entropy_drift.save_html(\"Multiclass_Confidence_Entropy_Drift.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb8d3f",
   "metadata": {},
   "source": [
    "#### Creating an easier overview for the prediction drift\n",
    "\n",
    "- Entropy:\n",
    "\n",
    "    Entropy measures how unsure the model is on average. This will allow us to catach patterns that the model has not seen before, erly concept drift etc. . If entropy increases, the model says that it is less certain that it used to be.\n",
    "\n",
    "- Max confidence:\n",
    "\n",
    "    Max confidence measures how extreme predictions are. This will allow us to track the model becoming confidently wrong, tracking distribution shift which causes probability inflation etc. . If max confidence increases unnaturally, the model may be very sure but no longer correct.\n",
    "\n",
    "- Margin\n",
    "\n",
    "    Margin measures how clearly the model separates its top choice from the runner-up. This allows us to observe if the decision boundries get blurry, if classes start overlapping, if performance drops before accuracy changes etc. . If margin decreases, the model is saying => it predicts the same class but is not sure why it is predicting the class any longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all these def-functions have been added\n",
    "\n",
    "def entropy_from_proba(P: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Shannon entropy per row. P shape = (n, K), rows sum to 1.\"\"\"\n",
    "    EPS = 1e-12\n",
    "    P = np.clip(P, EPS, 1.0)\n",
    "    return -np.sum(P * np.log(P), axis=1)\n",
    "\n",
    "def max_confidence(P: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Max probability per row.\"\"\"\n",
    "    return np.max(P, axis=1)\n",
    "\n",
    "def multiclass_margin(P: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"p_max - p_second_max per row.\"\"\"\n",
    "    sorted_p = np.sort(P, axis=1)\n",
    "    return sorted_p[:, -1] - sorted_p[:, -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old/new probabilities\n",
    "old_p1 = binary_model.predict_proba(old_train_binary)[:, 1]\n",
    "new_p1 = binary_model.predict_proba(new_data_binary)[:, 1]\n",
    "\n",
    "# convert to (n,2) to compute entropy consistently\n",
    "old_P = np.vstack([1 - old_p1, old_p1]).T\n",
    "new_P = np.vstack([1 - new_p1, new_p1]).T\n",
    "\n",
    "ref_df = pd.DataFrame({\n",
    "    \"p_class_1\": old_p1,\n",
    "    \"entropy\": entropy_from_proba(old_P),\n",
    "    \"max_conf\": max_confidence(old_P),\n",
    "    \"margin\": np.abs(old_p1 - 0.5)\n",
    "})\n",
    "\n",
    "cur_df = pd.DataFrame({\n",
    "    \"p_class_1\": new_p1,\n",
    "    \"entropy\": entropy_from_proba(new_P),\n",
    "    \"max_conf\": max_confidence(new_P),\n",
    "    \"margin\": np.abs(new_p1 - 0.5)\n",
    "})\n",
    "\n",
    "report = Report([\n",
    "    DataDriftPreset(num_method=\"wasserstein\", drift_share=0.25),  # 1/4 columns drifting triggers dataset drift\n",
    "    ValueDrift(column=\"p_class_1\", method=\"wasserstein\", threshold=0.1),\n",
    "    ValueDrift(column=\"entropy\", method=\"wasserstein\", threshold=0.1),\n",
    "    ValueDrift(column=\"max_conf\", method=\"wasserstein\", threshold=0.1),\n",
    "    ValueDrift(column=\"margin\", method=\"wasserstein\", threshold=0.1),\n",
    "])\n",
    "binary_all_signals = report.run(cur_df, ref_df)\n",
    "binary_all_signals.save_html(\"Binary_AllSignals_Drift.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c68a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference (old)\n",
    "old_P = multi_model.predict_proba(old_train_multi)  # shape (n, K)\n",
    "\n",
    "# current (new)\n",
    "new_P = multi_model.predict_proba(new_data_multiclass)\n",
    "\n",
    "K = old_P.shape[1]\n",
    "proba_cols = [f\"p_class_{i}\" for i in range(K)]\n",
    "\n",
    "ref_multi = pd.DataFrame(old_P, columns=proba_cols)\n",
    "cur_multi = pd.DataFrame(new_P, columns=proba_cols)\n",
    "\n",
    "# derived signals\n",
    "ref_multi[\"entropy\"] = entropy_from_proba(old_P)\n",
    "cur_multi[\"entropy\"] = entropy_from_proba(new_P)\n",
    "\n",
    "ref_multi[\"max_conf\"] = max_confidence(old_P)\n",
    "cur_multi[\"max_conf\"] = max_confidence(new_P)\n",
    "\n",
    "ref_multi[\"margin\"] = multiclass_margin(old_P)   # pmax - p2\n",
    "cur_multi[\"margin\"] = multiclass_margin(new_P)\n",
    "\n",
    "# predicted label frequency drift (regime change)\n",
    "ref_multi[\"pred_label\"] = np.argmax(old_P, axis=1).astype(str)\n",
    "cur_multi[\"pred_label\"] = np.argmax(new_P, axis=1).astype(str)\n",
    "\n",
    "all_cols = proba_cols + [\"entropy\", \"max_conf\", \"margin\", \"pred_label\"]\n",
    "\n",
    "report = Report([\n",
    "    DataDriftPreset(\n",
    "        num_method=\"wasserstein\",\n",
    "        cat_method=\"jensenshannon\",\n",
    "        cat_threshold=0.1,\n",
    "        drift_share=0.3\n",
    "    ),\n",
    "    ValueDrift(column=\"entropy\", method=\"wasserstein\", threshold=0.1),\n",
    "    ValueDrift(column=\"max_conf\", method=\"wasserstein\", threshold=0.1),\n",
    "    ValueDrift(column=\"margin\", method=\"wasserstein\", threshold=0.1),\n",
    "])\n",
    "multiclass_allsignal = report.run(cur_multi[all_cols], ref_multi[all_cols])\n",
    "multiclass_allsignal.save_html(\"Multi_AllSignals_Drift.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189e489",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook illustrates how you can track data drift of raw & transformed data (X & y features), but also tracking how the prediction drift with new data or overtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
